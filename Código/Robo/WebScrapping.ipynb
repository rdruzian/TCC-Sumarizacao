{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando: https://g1.globo.com/index/feed/pagina-1.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-1.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-2.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-2.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-3.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-3.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-4.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-4.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-5.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-5.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-6.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-6.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-7.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-7.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-8.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-8.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-9.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-9.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-10.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-10.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-11.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-11.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-12.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-12.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-13.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-13.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-14.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-14.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-15.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-15.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-16.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-16.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-17.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-17.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-18.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-18.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-19.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-19.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-20.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-20.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-21.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-21.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-22.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-22.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-23.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-23.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-24.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-24.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-25.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-25.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-26.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-26.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-27.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-27.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-28.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-28.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-29.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-29.ghtml: terminado\n",
      "Salvando...\n",
      "Processando: https://g1.globo.com/index/feed/pagina-30.ghtml...\n",
      "https://g1.globo.com/index/feed/pagina-30.ghtml: terminado\n",
      "Salvando...\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Inicialização de variáveis\n",
    "data = {}\n",
    "data['noticias'] = []\n",
    "d = {}\n",
    "d['noticia'] = []\n",
    "count = 0\n",
    "c = 1\n",
    "p = 1\n",
    "verMais = []\n",
    "noticia = \"\"\n",
    "categoria = \"\"\n",
    "\n",
    "# Função para abrir uma URL e retornar o html caso não haja erro\n",
    "def urls(url):\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    except HTTPError as e:\n",
    "        print(e)\n",
    "    except URLError:\n",
    "        print(\"Server down or incorrect domain\")\n",
    "    else:\n",
    "        return html\n",
    "    \n",
    "# Função que recebe um slice contento as URLs das notícias e a variável para montar o json.\n",
    "# Aqui é separado o título da notícia, a categoria e o texto. São feitas alguma verificações \n",
    "# para não ir dados nulos e por fim guardado na variável que será criado o json\n",
    "def pegaInfos(titulos, count, categoria, noticia):\n",
    "    data['noticias'] = []\n",
    "    for tag in titulos:\n",
    "        manchete = \"\"\n",
    "        noticia = \"\"\n",
    "        noti = \"\"\n",
    "        ##Pega a url da noticia na pagina prinicpal\n",
    "        manchete = tag.getText()\n",
    "        url = tag.get(\"href\")\n",
    "        if url != \"\" and type(url) is str:\n",
    "            resp = urls(url)\n",
    "            if resp != \"\" and type(resp) is not None:\n",
    "                noticias = BeautifulSoup(resp.read(), \"html.parser\")\n",
    "                #Busca a categoria da notícia\n",
    "                cate = noticias.findAll(\"div\", {\"class\": \"header-title-content\"})\n",
    "                #Monta um slice com todos os paragrafos da noticia\n",
    "                news = noticias.findAll(\"p\", {\"class\": \"content-text__container\"})\n",
    "                #Monta uma string com a noticia, já removendo os paragrafos em branco\n",
    "                noticia = \"\"\n",
    "                for n in news:\n",
    "                    noti= n.getText()\n",
    "                    if noti != \"\":\n",
    "                        noticia = noticia + noti\n",
    "                #Pega apenas o título da categoria\n",
    "                for c in cate:\n",
    "                    categoria = c.getText()\n",
    "                \n",
    "                if noticia != \"\":\n",
    "                    data['noticias'].append([{'título': manchete, 'categoria': categoria, 'texto': noticia}])\n",
    "                    count = count + 1\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    return data, count\n",
    "\n",
    "# #### Loop para criar as URLs e realizar a busca das notícias, será necessário no minímo 15000 notícias\n",
    "while(c <= 30):\n",
    "    verMais.append(\"https://g1.globo.com/index/feed/pagina-\" + str(c) + \".ghtml\")\n",
    "    c += 1\n",
    "    \n",
    "def salvaJson(data, count):\n",
    "    print(\"Salvando...\")\n",
    "    # #### Por fim é criado o json utlizando o encoder UTF-8 para reconhecer a gramática PT-BR \n",
    "    with open('C:\\\\Users\\\\renat\\\\TCC\\\\Código\\\\TCC1\\\\tcc1.json', 'a+', encoding='utf8') as outfile:\n",
    "        json.dump(data, outfile, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    with open('C:\\\\Users\\\\renat\\\\TCC\\\\Código\\\\TCC1\\\\log1.txt', 'w', encoding='utf8') as file:\n",
    "        file.write(\"Total noticias: \" + str(count))\n",
    "        \n",
    "# #### Com as URLs da célula anterior será feito busca das notícias, página a página\n",
    "for n in verMais:\n",
    "    print(\"Processando: \" + str(n) + \"...\")\n",
    "    res = BeautifulSoup(urls(n).read(),\"html.parser\")\n",
    "    ## Alterar essa linha para encontrar a tag principal e mais um atributo html para identificar o titulo da noticia\n",
    "    titulos = res.findAll(\"a\", {\"class\": \"feed-post-link gui-color-primary gui-color-hover\"})\n",
    "    data, count = pegaInfos(titulos, count, categoria, noticia)\n",
    "    print(n + \": terminado\")\n",
    "    # if p == 10:\n",
    "    #     salvaJson(data, count)\n",
    "    #     p = 1\n",
    "    # else:\n",
    "    #     p += 1\n",
    "    salvaJson(data, count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
