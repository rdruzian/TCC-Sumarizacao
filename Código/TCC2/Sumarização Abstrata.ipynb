{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importação das bibliotecas que serão utilizadas para a criação do algoritmo de Sumarização Abstrata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "import tensorflow_datasets as tfds\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregado o dataset com as notícias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('tcc1.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregado o modelo pela biblioteca Spacy para o idioma Português, e em seguida carregado os Stop Words para o Português"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = spacy.lang.pt.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pontos = []\n",
    "for i in string.punctuation:\n",
    "    pontos.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para realizar a limpeza dos texto, conversão para minuscula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frase_limpa(texto):\n",
    "    palavra = []\n",
    "    texto = texto.lower()\n",
    "    texto = nlp(texto)\n",
    "    for p in texto:\n",
    "        palavra.append(p.text)\n",
    "    texto = [p for p in palavra if p not in stopwords]\n",
    "    texto = ' '.join([i for i in texto])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_caracteres(texto):\n",
    "    texto = re.sub('[0-9!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]*', '', texto)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizado o split das notícias e guardado em um array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticia_frase = []\n",
    "for i in range(len(data.texto)):\n",
    "    noticia_frase.append(frase_limpa(data.texto[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticia_limpa = [remove_caracteres(nf) for nf in noticia_frase]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificação do texto de uma notícias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  a saída nelson teich ministério saúde reacendeu o alerta empresários o grau confiança brasil o combate pandemia coronavírus  executivos mostras desconforto a coordenação a pasta e a presidência república    última sextafeira  entrevista philipp schiemer  presidente mercedesbenz brasil e américa latina  jornal econômico deixou evidente frustração  o atraso retomada economia conta ausência ações coordenadas combate covid governos federal  estadual e municipal   tristeza o estamos vendo   disse  analistas mercado financeiro passam a estimar tombo   o pib auxílio emergencial  governo começa a pagar a parcela benefício sozinho  o presidente bgc liquidez  ermínio lucci  a declaração schiemer “ expressa a visão empresários credibilidade brasil ”    “ o fato ministros serem mandados  a foco gestão saúde brasil  unidade estratégia  há consonância o executivo federal e o executivo estados e municípios combater a pandemia ”  destacou lucci    o empresário destacou  países reabrindo a economia  brasil discutida a possibilidade decretar o chamado lockdown  regras rígidas isolamento social  apontando o atraso brasil combate doença  “  realmente  tira o incentivo empresário investir país  afeta a credibilidade público  o significa prática investimentos próximos trimestres ”  disse lucci   planejar o futuro a vulnerabilidade empresário brasileiro    “ o fato a gente lidar crise econômica e saúde precedentes últimos  anos  somado a crise política  entendimento poderes ente federação    abalar a confiança empresários ”  reiterou o presidente bgc    paulo castello branco  presidente associação brasileira importadores máquinas e equipamentos industriais  abimei   vê prejuízos investimentos     vemos fato  saídas mandetta e teich  confirma o brasil vivendo realmente pesadelo  investidores atentos o acontecendo brasil e atrapalha a retomada economia vinha situação difícil   afirma     havia cautela investidores pandemia e cenário político dificulta  o investidor estrangeiro e interno olha o brasil vê previsibilidade investir  pandemia política prejudicando a retomada investimentos   aponta   o ministro saúde teria autonomia conduzir a gestão problema  e coordenação estados e municípios   impacto ruim a imagem exterior o presidente associação brasileira indústria elétrica e eletrônica  abinee   humberto barbato  a troca comando ministério saúde crítico pandemia traz impactos a imagem brasil exterior    “ coordenação ruim delicado  acredito o brasil visto exterior país bons nomes exercer função  ministro saúde  ”  afirma barbato  “ lamentável tenha ocorrido a substituição  fico contente e a repercussão internacionalmente  ” avalia  entanto  o brasil seguir atrativo o investimento internacional a pandemia superada  “ a economia mundo  esqueço o potencial o brasil  o país oportunidades o mundo  ” fugindo incerteza pesquisa feita presidentes  ceos  sócios e diretores empresas amchambrasil mostra   associados acreditam a coordenação esferas público  setor empresarial e sociedade a ação importante enfrentar o coronavírus    sejam maiores  sondagem feita  e  abril  período luiz henrique mandetta balançava cargo   o ministro havia sido demitido e o sucessor  renunciado    a previsibilidade o rumo brasil enfrentamento pandemia retrai a disposição investimento empresários e afugenta investidores estrangeiros  toa  expectativas analistas mercado a economia apontam queda   pib e dólar casa r  ano    empresários esperem o trabalho ministério saúde tenha curso continuidade  visto funcionários carreira continuam atividade trocas ministros  a mudança processo decisório adia decisões  a troca mandetta teich   modificou expectativas o país vencer a pandemia   o mercado espera queda cerca   pib brasil  alinhado o resto mundo  a plano claro deixa dúvida o impacto crise sanitária economia   welber barral  sócio consultoria barral m jorge e exsecretário comércio exterior     imagine fundo estrangeiro analisando investir infraestrutura brasil  cálculos retorno o plano país a pandemia   novidades setores essenciais durante a pandemia distantes problema  o hospital coração alagoas franca expansão início a pandemia coronavírus  o investimento mantido  a insegurança aumentou gestores    “ conta ramo  fica impossível parar investimento  o mudou o grau incerteza e estresse conta clareza  plano uniformizado o país  o enfrentamento pandemia e o período póspandemia ”  afirmou o médico ricardo césar cavalcanti  proprietário hospital    negócio diretamente afetado crise saúde  cavalcanti enfatizou a necessidade o governo espelhar experiência países adotaram estratégia rígida conter a disseminação doença   crise passa solução médica   solução econômica  inverter ordem inútil  o tamanho dano econômico respeito medidas médicas forem tomadas       o a gente visto a comprovação  países ouviram a técnica  a médica  e adotaram medidas rigorosas  passaram isolamento social e retração econômica     peso costas  o brasil   procura ministro  o blog andréia sadi  o presidente jair bolsonaro manter o secretárioexecutivo pasta  general eduardo pazuello  interino seja assinada a mudança protocolo uso cloroquina   posse ministro '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noticia_limpa[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui Terminamos o pré processamento do texto das notícias, como será realizada a sumarização Abstrata não será necessário o uso do título da notícia.\n",
    "\n",
    "Agora iremos iniciar a Tokenização do Texto para em seguida usar ele como entrada para a rede neural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenização do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizador = tfds.features.text.SubwordTextEncoder.build_from_corpus(noticia_limpa, target_vocab_size=2**16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificação do tamanho do vocabulário criado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17706"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizador.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase_input = [tokenizador.encode(frase) for frase in noticia_frase]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalização do tamanho das notícias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tam_max = max([len(frase)] for frase in frase_input)\n",
    "tam_max = tam_max[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificação do tamanho da maior notícias em nosso dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4898"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tam_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizado o padding, preenchimento, das notícias com valor 0.\n",
    "\n",
    "Esse prenchimento é feito no final da notícia, com isso, não tem impacto no processamento futuro do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase_input = tf.keras.preprocessing.sequence.pad_sequences(frase_input, value = 0, padding='post', maxlen=tam_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "treino_inputs, teste_inputs, treino_label, teste_label = train_test_split(frase_input, titulo_input, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construção do modelo da rede"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class DCNN(keras.Model):\n",
    "    def __init__(self, vocab_size, emb_dim = 128, nb_filter = 50, ffn_units = 512, dropout_rate = 0.1, \n",
    "                 training = False, name=\"dcnn\"):\n",
    "        super(DCNN, self).__init__(name=name)\n",
    "        self.embbeding = layers.Embedding(vocab_size, emb_dim)\n",
    "        self.bigram = layers.Conv1D(filters=nb_filter, kernel_size=2, padding='same', activation='relu')\n",
    "        self.trigram = layers.Conv1D(filters=nb_filter, kernel_size=3, padding='same', activation='relu')\n",
    "        self.ngram = layers.Conv1D(filters=nb_filter, kernel_size=4, padding='same', activation='relu')\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        self.dense_1 = layers.Dense(units=ffn_units, activation='relu')\n",
    "        self.dropout = layers.Dropout(rate = dropout_rate)\n",
    "        self.last_dense = layers.Dense(units = vocab_size, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.embbeding(inputs)\n",
    "        x_1 = self.bigram(x)\n",
    "        x_1 = self.pool(x_1)\n",
    "        x_2 = self.trigram(x)\n",
    "        x_2 = self.pool(x_2)\n",
    "        x_3 = self.ngram(x)\n",
    "        x_3 = self.pool(x_3)\n",
    "        \n",
    "        merged = tf.concat([x_1, x_2, x_3], axis = -1)\n",
    "        merged = self.dense_1(merged)\n",
    "        merged = self.dropout(merged)\n",
    "        output = self.last_dense(merged)\n",
    "        \n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamento da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizador.vocab_size\n",
    "emb_dim = 100\n",
    "nb_filter = 50\n",
    "ffn_units = 128\n",
    "dropout_rate = 0.2\n",
    "nb_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dcnn = DCNN(vocab_size=vocab_size, emb_dim=emb_dim, nb_filter=nb_filter, ffn_units = ffn_units, dropout_rate = dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dcnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checkpoint = './'\n",
    "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "titulo = data.título"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "titulo_frase = []\n",
    "for i in range(len(data.título)):\n",
    "    titulo_frase.append(frase_limpa(data.título[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "titulo_limpo = [remove_caracteres(tf) for tf in titulo_frase]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "titulo_input = [tokenizador.encode(titulo) for titulo in titulo_limpo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "titulo_max = max([len(titulo)] for titulo in titulo_input)\n",
    "titulo_max = titulo_max[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "titulo_input = tf.keras.preprocessing.sequence.pad_sequences(titulo_input, value = 0, padding='post', maxlen=tam_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = Input(shape=(None, tam_max))\n",
    "encoder = LSTM(emb_dim, return_state=True)\n",
    "encoder_output, state_h, state_c = encoder(encoder_input)\n",
    "encoder_state = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(None, titulo_max))\n",
    "decoder_lstm = LSTM(emb_dim, return_sequences=True, return_state=True)\n",
    "decoder_output, _, _ = decoder_lstm(decoder_input, initial_state=encoder_state)\n",
    "decoder_dense = Dense(titulo_max, activation='softmax')\n",
    "decoder_output = decoder_dense(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_target_data = np.zeros((len(titulo_input), titulo_max), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, None, 4898)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, None, 31)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, 100), (None, 1999600     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, None, 100),  52800       input_4[0][0]                    \n",
      "                                                                 lstm_3[0][1]                     \n",
      "                                                                 lstm_3[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 31)     3131        lstm_4[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,055,531\n",
      "Trainable params: 2,055,531\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_input, decoder_input], decoder_output)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frase_input = np.array(frase_input)\n",
    "#titulo_input = np.array(titulo_input)\n",
    "#x = np.array([frase_input, titulo_input])\n",
    "#x = (np.array(frase_input), np.array(titulo_input))\n",
    "#y = np.array(titulo_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_tokens = len(np.unique(frase_input))\n",
    "num_decoder_tokens = len(np.unique(titulo_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16702\n",
      "1415\n"
     ]
    }
   ],
   "source": [
    "print(num_encoder_tokens)\n",
    "print(num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 87.5 GiB for an array with shape (287, 4898, 16702) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-caabfb1e07da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m encoder_input_data = np.zeros(\n\u001b[0;32m      2\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrase_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtam_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_encoder_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     dtype='float32')\n\u001b[0m\u001b[0;32m      4\u001b[0m decoder_input_data = np.zeros(\n\u001b[0;32m      5\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrase_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitulo_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_decoder_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 87.5 GiB for an array with shape (287, 4898, 16702) and data type float32"
     ]
    }
   ],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(frase_input), tam_max, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(frase_input), titulo_max, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(frase_input), titulo_max, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder_input_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-44c5066366d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mencoder_input_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_input_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_target_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'encoder_input_data' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=64, epochs=nb_epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
