{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importação das bibliotecas que serão utilizadas para a criação do algoritmo de Sumarização Abstrata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "import tensorflow_datasets as tfds\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregado o dataset com as notícias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('tcc1.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificação do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>título</th>\n",
       "      <th>categoria</th>\n",
       "      <th>texto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Falta de política clara para conter pandemia a...</td>\n",
       "      <td>Economia</td>\n",
       "      <td>A saída de Nelson Teich do Ministério da Saúd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brasil chega a 16.792 mortes e se torna 3º do ...</td>\n",
       "      <td>Bem Estar</td>\n",
       "      <td>O Ministério da Saúde divulgou nesta segunda-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MPF investigará se Flávio Bolsonaro foi avisad...</td>\n",
       "      <td>Rio de Janeiro</td>\n",
       "      <td>O Ministério Público Federal informou nesta s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Advogado investigado por 'rachadinha' é suspei...</td>\n",
       "      <td>Rio de Janeiro</td>\n",
       "      <td>Investigado no caso das chamadas \"rachadinhas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Celso de Mello diz que decidirá sobre sigilo d...</td>\n",
       "      <td>Política</td>\n",
       "      <td>O Supremo Tribunal Federal (STF) informou nes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              título         categoria  \\\n",
       "0  Falta de política clara para conter pandemia a...         Economia    \n",
       "1  Brasil chega a 16.792 mortes e se torna 3º do ...        Bem Estar    \n",
       "2  MPF investigará se Flávio Bolsonaro foi avisad...   Rio de Janeiro    \n",
       "3  Advogado investigado por 'rachadinha' é suspei...   Rio de Janeiro    \n",
       "4  Celso de Mello diz que decidirá sobre sigilo d...         Política    \n",
       "\n",
       "                                               texto  \n",
       "0   A saída de Nelson Teich do Ministério da Saúd...  \n",
       "1   O Ministério da Saúde divulgou nesta segunda-...  \n",
       "2   O Ministério Público Federal informou nesta s...  \n",
       "3   Investigado no caso das chamadas \"rachadinhas...  \n",
       "4   O Supremo Tribunal Federal (STF) informou nes...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregado o modelo pela biblioteca Spacy para o idioma Português, e em seguida carregado os Stop Words para o Português"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.pt.Portuguese at 0x20b46f82a88>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('pt_core_news_sm')\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = spacy.lang.pt.STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para realizar a limpeza dos texto, conversão para minuscula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frase_limpa(texto):\n",
    "    palavra = []\n",
    "    texto = texto.lower()\n",
    "    texto = nlp(texto)\n",
    "    for p in texto:\n",
    "        palavra.append(p.text)\n",
    "    texto = [p for p in palavra if p not in stopwords]\n",
    "    texto = ' '.join([i for i in texto])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizado o split das notícias e guardado em um array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticia_frase = []\n",
    "for i in range(len(data.texto)):\n",
    "    noticia_frase.append(frase_limpa(data.texto[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificação do texto de uma notícias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  a saída nelson teich ministério saúde reacendeu o alerta empresários o grau confiança brasil o combate pandemia coronavírus . executivos mostras desconforto a coordenação a pasta e a presidência república .   última sexta-feira , entrevista philipp schiemer , presidente mercedes-benz brasil e américa latina , jornal econômico deixou evidente frustração : o atraso retomada economia conta ausência ações coordenadas combate covid-19 governos federal , estadual e municipal . \" tristeza o estamos vendo \" , disse . analistas mercado financeiro passam a estimar tombo 5,12 % o pib 2020auxílio emergencial : governo começa a pagar a parcela benefício sozinho . o presidente bgc liquidez , ermínio lucci , a declaração schiemer “ expressa a visão empresários credibilidade brasil ” .   “ o fato ministros serem mandados , a foco gestão saúde brasil , unidade estratégia , há consonância o executivo federal e o executivo estados e municípios combater a pandemia ” , destacou lucci .   o empresário destacou , países reabrindo a economia , brasil discutida a possibilidade decretar o chamado lockdown , regras rígidas isolamento social , apontando o atraso brasil combate doença . “ , realmente , tira o incentivo empresário investir país . afeta a credibilidade público . o significa prática investimentos próximos trimestres ” , disse lucci . , planejar o futuro a vulnerabilidade empresário brasileiro .   “ o fato a gente lidar crise econômica e saúde precedentes últimos 100 anos , somado a crise política , entendimento poderes ente federação , , , abalar a confiança empresários ” , reiterou o presidente bgc .   paulo castello branco , presidente associação brasileira importadores máquinas e equipamentos industriais ( abimei ) , vê prejuízos investimentos .   \" vemos fato ( saídas mandetta e teich ) confirma o brasil vivendo realmente pesadelo . investidores atentos o acontecendo brasil e atrapalha a retomada economia vinha situação difícil \" , afirma .   \" havia cautela investidores pandemia e cenário político dificulta . o investidor estrangeiro e interno olha o brasil vê previsibilidade investir . pandemia política prejudicando a retomada investimentos \" , aponta . \" o ministro saúde teria autonomia conduzir a gestão problema . e coordenação estados e municípios \" . impacto ruim a imagem exterior o presidente associação brasileira indústria elétrica e eletrônica ( abinee ) , humberto barbato , a troca comando ministério saúde crítico pandemia traz impactos a imagem brasil exterior .   “ coordenação ruim delicado . acredito o brasil visto exterior país bons nomes exercer função ( ministro saúde ) ” , afirma barbato . “ lamentável tenha ocorrido a substituição , fico contente e a repercussão internacionalmente . ” avalia , entanto , o brasil seguir atrativo o investimento internacional a pandemia superada . “ a economia mundo . esqueço o potencial o brasil . o país oportunidades o mundo . ” fugindo incerteza pesquisa feita presidentes , ceos , sócios e diretores empresas amcham-brasil mostra 47 % associados acreditam a coordenação esferas público , setor empresarial e sociedade a ação importante enfrentar o coronavírus .   sejam maiores , sondagem feita 8 e 19 abril , período luiz henrique mandetta balançava cargo . , o ministro havia sido demitido e o sucessor , renunciado .   a previsibilidade o rumo brasil enfrentamento pandemia retrai a disposição investimento empresários e afugenta investidores estrangeiros . toa , expectativas analistas mercado a economia apontam queda 5,12 % pib e dólar casa r$ 5,28 ano .   empresários esperem o trabalho ministério saúde tenha curso continuidade , visto funcionários carreira continuam atividade trocas ministros , a mudança processo decisório adia decisões . a troca mandetta teich , , modificou expectativas o país vencer a pandemia . \" o mercado espera queda cerca 5 % pib brasil , alinhado o resto mundo . a plano claro deixa dúvida o impacto crise sanitária economia \" , welber barral , sócio consultoria barral m jorge e ex-secretário comércio exterior .   \" imagine fundo estrangeiro analisando investir infraestrutura brasil . cálculos retorno o plano país a pandemia ? \" novidades setores essenciais durante a pandemia distantes problema . o hospital coração alagoas franca expansão início a pandemia coronavírus . o investimento mantido , a insegurança aumentou gestores .   “ conta ramo , fica impossível parar investimento . o mudou o grau incerteza e estresse conta clareza , plano uniformizado o país , o enfrentamento pandemia e o período pós-pandemia ” , afirmou o médico ricardo césar cavalcanti , proprietário hospital .   negócio diretamente afetado crise saúde , cavalcanti enfatizou a necessidade o governo espelhar experiência países adotaram estratégia rígida conter a disseminação doença . \" crise passa solução médica , , solução econômica . inverter ordem inútil . o tamanho dano econômico respeito medidas médicas forem tomadas \" , .   \" o a gente visto a comprovação , países ouviram a técnica , a médica , e adotaram medidas rigorosas , passaram isolamento social e retração econômica . \"   peso costas , o brasil , , procura ministro . o blog andréia sadi , o presidente jair bolsonaro manter o secretário-executivo pasta , general eduardo pazuello , interino seja assinada a mudança protocolo uso cloroquina , , posse ministro .'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noticia_frase[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui Terminamos o pré processamento do texto das notícias, como será realizada a sumarização Abstrata não será necessário o uso do título da notícia.\n",
    "\n",
    "Agora iremos iniciar a Tokenização do Texto para em seguida usar ele como entrada para a rede neural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenização do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizador = tfds.features.text.SubwordTextEncoder.build_from_corpus(noticia_frase, target_vocab_size=2**16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificação do tamanho do vocabulário criado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18666"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizador.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase_input = [tokenizador.encode(frase) for frase in noticia_frase]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalização do tamanho das notícias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tam_max = max([len(frase)] for frase in frase_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificação do tamanho da maior notícias em nosso dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tam_max = tam_max[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizado o padding, preenchimento, das notícias com valor 0.\n",
    "\n",
    "Esse prenchimento é feito no final da notícia, com isso, não tem impacto no processamento futuro do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase_input = tf.keras.preprocessing.sequence.pad_sequences(frase_input, value = 0, padding='post', maxlen=tam_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3022"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frase_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "treino_inputs, teste_inputs, treino_label, teste_label = train_test_split(frase_input, titulo_input, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construção do modelo da rede"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class DCNN(keras.Model):\n",
    "    def __init__(self, vocab_size, emb_dim = 128, nb_filter = 50, ffn_units = 512, dropout_rate = 0.1, \n",
    "                 training = False, name=\"dcnn\"):\n",
    "        super(DCNN, self).__init__(name=name)\n",
    "        self.embbeding = layers.Embedding(vocab_size, emb_dim)\n",
    "        self.bigram = layers.Conv1D(filters=nb_filter, kernel_size=2, padding='same', activation='relu')\n",
    "        self.trigram = layers.Conv1D(filters=nb_filter, kernel_size=3, padding='same', activation='relu')\n",
    "        self.ngram = layers.Conv1D(filters=nb_filter, kernel_size=4, padding='same', activation='relu')\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        self.dense_1 = layers.Dense(units=ffn_units, activation='relu')\n",
    "        self.dropout = layers.Dropout(rate = dropout_rate)\n",
    "        self.last_dense = layers.Dense(units = vocab_size, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.embbeding(inputs)\n",
    "        x_1 = self.bigram(x)\n",
    "        x_1 = self.pool(x_1)\n",
    "        x_2 = self.trigram(x)\n",
    "        x_2 = self.pool(x_2)\n",
    "        x_3 = self.ngram(x)\n",
    "        x_3 = self.pool(x_3)\n",
    "        \n",
    "        merged = tf.concat([x_1, x_2, x_3], axis = -1)\n",
    "        merged = self.dense_1(merged)\n",
    "        merged = self.dropout(merged)\n",
    "        output = self.last_dense(merged)\n",
    "        \n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamento da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizador.vocab_size\n",
    "emb_dim = 200\n",
    "nb_filter = 100\n",
    "ffn_units = 256\n",
    "dropout_rate = 0.2\n",
    "nb_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dcnn = DCNN(vocab_size=vocab_size, emb_dim=emb_dim, nb_filter=nb_filter, ffn_units = ffn_units, dropout_rate = dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dcnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checkpoint = './'\n",
    "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "titulo = data.título"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "titulo_frase = []\n",
    "for i in range(len(data.título)):\n",
    "    titulo_frase.append(frase_limpa(data.título[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "titulo_input = [tokenizador.encode(titulo) for titulo in titulo_frase]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "titulo_max = max([len(titulo)] for titulo in titulo_input)\n",
    "titulo_max = titulo_max[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase_input = tf.keras.preprocessing.sequence.pad_sequences(titulo_input, value = 0, padding='post', maxlen=titulo_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = Input(shape=(None, tam_max))\n",
    "encoder = LSTM(emb_dim, return_state=True)\n",
    "encoder_output, state_h, state_c = encoder(encoder_input)\n",
    "encoder_state = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(None, titulo_max))\n",
    "decoder_lstm = LSTM(emb_dim, return_sequences=True, return_state=True)\n",
    "decoder_output, _, _ = decoder_lstm(decoder_input, initial_state=encoder_state)\n",
    "decoder_dense = Dense(50, activation='softmax')\n",
    "decoder_output = decoder_dense(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-637de071eea8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mencoder_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_input\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rmsprop'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfrase_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitulo_input\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitulo_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\TCC\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\TCC\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\TCC\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'DataFrame'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\TCC\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'DataFrame'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\TCC\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_single_array\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     32\u001b[0m                 'Got tensor with shape: %s' % str(shape))\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[1;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_input, decoder_input], decoder_output)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit([frase_input, titulo_input], titulo_input, batch_size=64, epochs=nb_epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
