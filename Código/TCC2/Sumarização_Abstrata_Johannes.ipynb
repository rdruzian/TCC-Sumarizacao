{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_J8EJ1hqdVEJ"
   },
   "source": [
    "Importação das bibliotecas que serão utilizadas para a criação do algoritmo de Sumarização Abstrata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ebbZQqKZdVEK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In C:\\Users\\renat\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\renat\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\renat\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In C:\\Users\\renat\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\renat\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\renat\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\renat\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\renat\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "import tensorflow_datasets as tfds\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Amk2yS2Rl9Wu"
   },
   "outputs": [],
   "source": [
    "def tokenize(lang, max_seq):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=True, filters='',\n",
    "                                                         num_words=2**16)\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post',\n",
    "                                                         maxlen=max_seq)\n",
    "\n",
    "  return tensor, lang_tokenizer\n",
    "\n",
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRgMGI5pdVEN"
   },
   "source": [
    "Carregado o dataset com as notícias, separando o texto e o título"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KbfDZKwMdVEN"
   },
   "outputs": [],
   "source": [
    "data = pd.read_json('tcc1.json', encoding='utf-8')\n",
    "\n",
    "noticias = ['<start> ' + m + ' <end>' for m in data.texto.tolist()]\n",
    "titulos  = ['<start> ' + m + ' <end>' for m in data.título.tolist()]\n",
    "\n",
    "input_tensor, inp_lang = tokenize(noticias, max_seq=600)\n",
    "target_tensor, targ_lang = tokenize(titulos, max_seq=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "GwlLqXmQC7aU",
    "outputId": "f3739a60-ec96-4628-e285-7dc5c5b5d45e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_lang.word_index['<start>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SYDsPfw5mlqR"
   },
   "outputs": [],
   "source": [
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "g88lLJYIdVFR",
    "outputId": "ff8f85d7-9aa3-4ee1-cea8-7161c4d67dc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229 229 58 58\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# faz separacao 80/20\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# mostra tamanhos\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lnvQHQXsm7u6",
    "outputId": "a7aa0faa-53ec-4180-c9b6-08ef26071f32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "18 ----> por\n",
      "421 ----> exemplo,\n",
      "4 ----> e\n",
      "74 ----> isso\n",
      "17 ----> não\n",
      "16 ----> é\n",
      "1587 ----> necessariamente\n",
      "4072 ----> associado\n",
      "2 ----> a\n",
      "88 ----> essa\n",
      "533 ----> doença.\n",
      "271 ----> essas\n",
      "45 ----> pessoas\n",
      "1115 ----> continuam\n",
      "2477 ----> circulando\n",
      "4 ----> e\n",
      "4073 ----> transmitindo\n",
      "3 ----> o\n",
      "4074 ----> vírus\",\n",
      "150 ----> afirma\n",
      "3373 ----> silva.\n",
      "3 ----> o\n",
      "3419 ----> epidemiologista\n",
      "7189 ----> lúcio\n",
      "7190 ----> botelho,\n",
      "574 ----> professor\n",
      "6 ----> do\n",
      "763 ----> departamento\n",
      "1 ----> de\n",
      "48 ----> saúde\n",
      "250 ----> pública\n",
      "8 ----> da\n",
      "389 ----> universidade\n",
      "107 ----> federal\n",
      "1 ----> de\n",
      "1012 ----> santa\n",
      "2478 ----> catarina\n",
      "7191 ----> (ufsc),\n",
      "2904 ----> considera\n",
      "113 ----> esse\n",
      "178 ----> índice\n",
      "7192 ----> \"assustador\".\n",
      "849 ----> especialmente\n",
      "138 ----> porque\n",
      "117 ----> apenas\n",
      "7193 ----> 2,55%\n",
      "22 ----> dos\n",
      "1050 ----> participantes\n",
      "8 ----> da\n",
      "287 ----> pesquisa\n",
      "5210 ----> afirmaram\n",
      "64 ----> ter\n",
      "152 ----> sido\n",
      "4075 ----> testadas\n",
      "9 ----> para\n",
      "105 ----> covid-19\n",
      "10 ----> com\n",
      "1076 ----> exames\n",
      "7194 ----> moleculares,\n",
      "5 ----> que\n",
      "26 ----> são\n",
      "14 ----> os\n",
      "5 ----> que\n",
      "3431 ----> permitem\n",
      "7195 ----> diagnosticar\n",
      "2 ----> a\n",
      "533 ----> doença.\n",
      "7196 ----> \"sem\n",
      "993 ----> conseguir\n",
      "89 ----> fazer\n",
      "238 ----> testes\n",
      "7 ----> em\n",
      "2166 ----> massa,\n",
      "2 ----> a\n",
      "82 ----> gente\n",
      "17 ----> não\n",
      "38 ----> tem\n",
      "23 ----> como\n",
      "917 ----> saber\n",
      "124 ----> quem\n",
      "46 ----> está\n",
      "4073 ----> transmitindo\n",
      "3 ----> o\n",
      "213 ----> vírus\n",
      "34 ----> ou\n",
      "2177 ----> não.\n",
      "74 ----> isso\n",
      "864 ----> significa\n",
      "5 ----> que\n",
      "3 ----> o\n",
      "94 ----> isolamento\n",
      "16 ----> é\n",
      "1588 ----> fundamental,\n",
      "138 ----> porque\n",
      "16 ----> é\n",
      "2 ----> a\n",
      "656 ----> única\n",
      "224 ----> forma\n",
      "5 ----> que\n",
      "541 ----> temos\n",
      "364 ----> hoje\n",
      "1 ----> de\n",
      "1581 ----> impedir\n",
      "5 ----> que\n",
      "74 ----> isso\n",
      "7197 ----> aconteça\",\n",
      "150 ----> afirma\n",
      "7198 ----> botelho.\n",
      "1574 ----> perguntas\n",
      "7 ----> em\n",
      "1310 ----> aberto\n",
      "18 ----> por\n",
      "1914 ----> fim,\n",
      "3 ----> o\n",
      "303 ----> estudo\n",
      "847 ----> espanhol\n",
      "722 ----> aponta\n",
      "110 ----> que,\n",
      "56 ----> entre\n",
      "19 ----> as\n",
      "45 ----> pessoas\n",
      "5 ----> que\n",
      "1077 ----> haviam\n",
      "227 ----> feito\n",
      "13 ----> um\n",
      "746 ----> teste\n",
      "7199 ----> molecular\n",
      "9 ----> para\n",
      "192 ----> covid-19,\n",
      "7200 ----> 87%\n",
      "750 ----> tinham\n",
      "552 ----> anticorpos\n",
      "87 ----> contra\n",
      "3 ----> o\n",
      "5211 ----> sars-cov-2.\n",
      "74 ----> isso\n",
      "16 ----> é\n",
      "13 ----> um\n",
      "474 ----> bom\n",
      "4076 ----> sinal,\n",
      "138 ----> porque\n",
      "722 ----> aponta\n",
      "5 ----> que\n",
      "408 ----> nosso\n",
      "2479 ----> corpo\n",
      "3432 ----> desenvolve\n",
      "1930 ----> alguma\n",
      "224 ----> forma\n",
      "1 ----> de\n",
      "544 ----> proteção\n",
      "87 ----> contra\n",
      "113 ----> esse\n",
      "71 ----> novo\n",
      "802 ----> vírus.\n",
      "12 ----> na\n",
      "1458 ----> pesquisa,\n",
      "54 ----> foram\n",
      "1589 ----> analisados\n",
      "117 ----> apenas\n",
      "14 ----> os\n",
      "552 ----> anticorpos\n",
      "7201 ----> conhecidos\n",
      "10 ----> com\n",
      "4077 ----> igg,\n",
      "5 ----> que\n",
      "26 ----> são\n",
      "1007 ----> aqueles\n",
      "2438 ----> criados\n",
      "9 ----> para\n",
      "5 ----> que\n",
      "3 ----> o\n",
      "3353 ----> organismo\n",
      "282 ----> seja\n",
      "2170 ----> capaz\n",
      "11 ----> no\n",
      "1402 ----> futuro\n",
      "1 ----> de\n",
      "1401 ----> combater\n",
      "15 ----> uma\n",
      "594 ----> mesma\n",
      "1584 ----> ameaça\n",
      "1 ----> de\n",
      "224 ----> forma\n",
      "20 ----> mais\n",
      "4078 ----> eficiente.\n",
      "3 ----> o\n",
      "2897 ----> imunologista\n",
      "2475 ----> renato\n",
      "4079 ----> astray\n",
      "1457 ----> avalia\n",
      "5 ----> que\n",
      "3 ----> o\n",
      "178 ----> índice\n",
      "1 ----> de\n",
      "3433 ----> 13%\n",
      "5 ----> que\n",
      "17 ----> não\n",
      "2905 ----> desenvolveram\n",
      "552 ----> anticorpos\n",
      "106 ----> pode\n",
      "64 ----> ter\n",
      "152 ----> sido\n",
      "2480 ----> causado\n",
      "18 ----> por\n",
      "13 ----> um\n",
      "4080 ----> erro\n",
      "1 ----> de\n",
      "2906 ----> diagnóstico,\n",
      "138 ----> porque\n",
      "14 ----> os\n",
      "238 ----> testes\n",
      "2168 ----> rápidos\n",
      "2883 ----> usados\n",
      "47 ----> nesta\n",
      "366 ----> fase\n",
      "6 ----> do\n",
      "303 ----> estudo\n",
      "1659 ----> dão\n",
      "519 ----> resultados\n",
      "10 ----> com\n",
      "13 ----> um\n",
      "178 ----> índice\n",
      "1 ----> de\n",
      "3434 ----> precisão\n",
      "1 ----> de\n",
      "117 ----> apenas\n",
      "7202 ----> 79%.\n",
      "4079 ----> astray\n",
      "70 ----> diz\n",
      "5 ----> que\n",
      "108 ----> só\n",
      "121 ----> será\n",
      "275 ----> possível\n",
      "5212 ----> confirmar\n",
      "34 ----> ou\n",
      "7203 ----> refutar\n",
      "88 ----> essa\n",
      "7204 ----> hipótese\n",
      "51 ----> nas\n",
      "1937 ----> próximas\n",
      "3435 ----> etapas\n",
      "8 ----> da\n",
      "1458 ----> pesquisa,\n",
      "59 ----> quando\n",
      "121 ----> será\n",
      "227 ----> feito\n",
      "13 ----> um\n",
      "356 ----> tipo\n",
      "1 ----> de\n",
      "746 ----> teste\n",
      "1 ----> de\n",
      "552 ----> anticorpos\n",
      "5213 ----> laboratorial\n",
      "5 ----> que\n",
      "38 ----> tem\n",
      "15 ----> uma\n",
      "3434 ----> precisão\n",
      "5214 ----> maior,\n",
      "7 ----> em\n",
      "2145 ----> torno\n",
      "1 ----> de\n",
      "3436 ----> 95%\n",
      "33 ----> também\n",
      "121 ----> será\n",
      "1155 ----> necessário\n",
      "89 ----> fazer\n",
      "20 ----> mais\n",
      "893 ----> pesquisas\n",
      "9 ----> para\n",
      "2167 ----> compreender\n",
      "25 ----> se\n",
      "88 ----> essa\n",
      "346 ----> resposta\n",
      "2116 ----> imunológica\n",
      "16 ----> é\n",
      "2358 ----> realmente\n",
      "4078 ----> eficiente.\n",
      "7205 ----> \"ter\n",
      "552 ----> anticorpos\n",
      "17 ----> não\n",
      "1587 ----> necessariamente\n",
      "864 ----> significa\n",
      "381 ----> estar\n",
      "7206 ----> protegido,\n",
      "138 ----> porque\n",
      "113 ----> esse\n",
      "2178 ----> anticorpo\n",
      "359 ----> precisa\n",
      "41 ----> ser\n",
      "6 ----> do\n",
      "356 ----> tipo\n",
      "7207 ----> neutralizante\n",
      "9 ----> para\n",
      "1581 ----> impedir\n",
      "3 ----> o\n",
      "213 ----> vírus\n",
      "1 ----> de\n",
      "3427 ----> infectar\n",
      "2 ----> a\n",
      "7208 ----> célula\",\n",
      "150 ----> afirma\n",
      "3 ----> o\n",
      "4068 ----> imunologista.\n",
      "3 ----> o\n",
      "4081 ----> virologista\n",
      "7209 ----> aguinaldo\n",
      "5215 ----> pinto,\n",
      "574 ----> professor\n",
      "6 ----> do\n",
      "763 ----> departamento\n",
      "1 ----> de\n",
      "7210 ----> microbiologia,\n",
      "2481 ----> imunologia\n",
      "4 ----> e\n",
      "3437 ----> parasitologia\n",
      "8 ----> da\n",
      "7211 ----> ufsc,\n",
      "362 ----> explica\n",
      "5 ----> que\n",
      "74 ----> isso\n",
      "17 ----> não\n",
      "5171 ----> ocorre,\n",
      "18 ----> por\n",
      "421 ----> exemplo,\n",
      "10 ----> com\n",
      "3 ----> o\n",
      "213 ----> vírus\n",
      "7212 ----> hiv.\n",
      "1236 ----> \"uma\n",
      "335 ----> pessoa\n",
      "5 ----> que\n",
      "38 ----> tem\n",
      "3438 ----> hiv\n",
      "38 ----> tem\n",
      "15 ----> uma\n",
      "2907 ----> quantidade\n",
      "2908 ----> enorme\n",
      "1 ----> de\n",
      "552 ----> anticorpos\n",
      "4077 ----> igg,\n",
      "32 ----> mas\n",
      "151 ----> eles\n",
      "401 ----> nunca\n",
      "26 ----> são\n",
      "7213 ----> neutralizantes,\n",
      "4 ----> e\n",
      "3 ----> o\n",
      "1003 ----> paciente\n",
      "3432 ----> desenvolve\n",
      "2 ----> a\n",
      "5216 ----> aids\n",
      "25 ----> se\n",
      "17 ----> não\n",
      "5217 ----> fizer\n",
      "3 ----> o\n",
      "7214 ----> tratamento\",\n",
      "150 ----> afirma\n",
      "2909 ----> pinto.\n",
      "326 ----> outro\n",
      "1745 ----> aspecto\n",
      "405 ----> importante\n",
      "16 ----> é\n",
      "1209 ----> verificar\n",
      "25 ----> se\n",
      "88 ----> essa\n",
      "1737 ----> imunidade\n",
      "16 ----> é\n",
      "1 ----> de\n",
      "2891 ----> curto\n",
      "34 ----> ou\n",
      "453 ----> longo\n",
      "4052 ----> prazo.\n",
      "3 ----> o\n",
      "4081 ----> virologista\n",
      "150 ----> afirma\n",
      "110 ----> que,\n",
      "80 ----> mesmo\n",
      "59 ----> quando\n",
      "7215 ----> desenvolvemos\n",
      "13 ----> um\n",
      "7216 ----> anticorpo,\n",
      "40 ----> ele\n",
      "17 ----> não\n",
      "25 ----> se\n",
      "2138 ----> mantém\n",
      "1587 ----> necessariamente\n",
      "9 ----> para\n",
      "318 ----> sempre\n",
      "11 ----> no\n",
      "7217 ----> organismo.\n",
      "1945 ----> \"isso\n",
      "1159 ----> acontece\n",
      "10 ----> com\n",
      "2 ----> a\n",
      "7218 ----> caxumba,\n",
      "18 ----> por\n",
      "421 ----> exemplo,\n",
      "5 ----> que\n",
      "1312 ----> gera\n",
      "15 ----> uma\n",
      "2910 ----> memória\n",
      "2116 ----> imunológica\n",
      "1 ----> de\n",
      "453 ----> longo\n",
      "5218 ----> prazo,\n",
      "32 ----> mas\n",
      "17 ----> não\n",
      "10 ----> com\n",
      "3 ----> o\n",
      "7219 ----> rotavírus,\n",
      "5 ----> que\n",
      "194 ----> causa\n",
      "7220 ----> diarreia\n",
      "7 ----> em\n",
      "15 ----> uma\n",
      "4082 ----> criança.\n",
      "3 ----> o\n",
      "2178 ----> anticorpo\n",
      "87 ----> contra\n",
      "40 ----> ele\n",
      "5219 ----> dura\n",
      "18 ----> por\n",
      "13 ----> um\n",
      "247 ----> tempo\n",
      "4 ----> e\n",
      "126 ----> depois\n",
      "7221 ----> desaparece.\n",
      "108 ----> só\n",
      "357 ----> vamos\n",
      "1868 ----> descobrir\n",
      "7 ----> em\n",
      "354 ----> qual\n",
      "187 ----> caso\n",
      "3 ----> o\n",
      "73 ----> coronavírus\n",
      "25 ----> se\n",
      "7222 ----> encaixa\n",
      "10 ----> com\n",
      "3 ----> o\n",
      "1063 ----> passar\n",
      "6 ----> do\n",
      "3439 ----> tempo\",\n",
      "150 ----> afirma\n",
      "2909 ----> pinto.\n",
      "33 ----> também\n",
      "121 ----> será\n",
      "575 ----> preciso\n",
      "89 ----> fazer\n",
      "20 ----> mais\n",
      "893 ----> pesquisas\n",
      "9 ----> para\n",
      "2167 ----> compreender\n",
      "13 ----> um\n",
      "1745 ----> aspecto\n",
      "7223 ----> intrigante\n",
      "22 ----> dos\n",
      "142 ----> dados\n",
      "5220 ----> apresentados\n",
      "44 ----> até\n",
      "241 ----> agora\n",
      "31 ----> pelo\n",
      "303 ----> estudo\n",
      "2911 ----> espanhol.\n",
      "2 ----> a\n",
      "129 ----> taxa\n",
      "1 ----> de\n",
      "1050 ----> participantes\n",
      "10 ----> com\n",
      "552 ----> anticorpos\n",
      "26 ----> são\n",
      "280 ----> bem\n",
      "440 ----> diferentes\n",
      "56 ----> entre\n",
      "19 ----> as\n",
      "850 ----> faixas\n",
      "7224 ----> etárias.\n",
      "14 ----> os\n",
      "714 ----> índices\n",
      "7 ----> em\n",
      "232 ----> crianças\n",
      "1 ----> de\n",
      "2126 ----> 0\n",
      "4 ----> e\n",
      "481 ----> 9\n",
      "90 ----> anos\n",
      "5221 ----> variam\n",
      "56 ----> entre\n",
      "4062 ----> 1,1%\n",
      "4 ----> e\n",
      "4083 ----> 3%\n",
      "4 ----> e\n",
      "26 ----> são\n",
      "7225 ----> sensivelmente\n",
      "2179 ----> menores\n",
      "6 ----> do\n",
      "5 ----> que\n",
      "56 ----> entre\n",
      "14 ----> os\n",
      "2482 ----> idosos,\n",
      "5 ----> que\n",
      "3440 ----> apresentam\n",
      "1733 ----> taxas\n",
      "56 ----> entre\n",
      "5222 ----> 5,1%\n",
      "4 ----> e\n",
      "7226 ----> 6,9%.\n",
      "257 ----> \"o\n",
      "1711 ----> esperado\n",
      "267 ----> seria\n",
      "5 ----> que\n",
      "2 ----> a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "839 ----> maioria\n",
      "22 ----> dos\n",
      "488 ----> grupos\n",
      "2912 ----> tivessem\n",
      "2 ----> a\n",
      "594 ----> mesma\n",
      "285 ----> produção\n",
      "1 ----> de\n",
      "7227 ----> anticorpos\",\n",
      "70 ----> diz\n",
      "2909 ----> pinto.\n",
      "15 ----> uma\n",
      "7228 ----> explicação\n",
      "275 ----> possível\n",
      "16 ----> é\n",
      "5 ----> que\n",
      "3 ----> o\n",
      "140 ----> sistema\n",
      "2483 ----> imunológico\n",
      "1 ----> de\n",
      "232 ----> crianças\n",
      "62 ----> muito\n",
      "4084 ----> jovens\n",
      "49 ----> ainda\n",
      "46 ----> está\n",
      "7 ----> em\n",
      "2913 ----> formação,\n",
      "3 ----> o\n",
      "5 ----> que\n",
      "4085 ----> levaria\n",
      "2 ----> a\n",
      "15 ----> uma\n",
      "804 ----> menor\n",
      "285 ----> produção\n",
      "1 ----> de\n",
      "2894 ----> anticorpos.\n",
      "32 ----> mas\n",
      "74 ----> isso\n",
      "33 ----> também\n",
      "1072 ----> deveria\n",
      "41 ----> ser\n",
      "972 ----> visto\n",
      "7 ----> em\n",
      "2482 ----> idosos,\n",
      "138 ----> porque\n",
      "408 ----> nosso\n",
      "140 ----> sistema\n",
      "2483 ----> imunológico\n",
      "496 ----> começa\n",
      "2 ----> a\n",
      "25 ----> se\n",
      "7229 ----> deteriorar\n",
      "2 ----> a\n",
      "260 ----> partir\n",
      "22 ----> dos\n",
      "576 ----> 60\n",
      "205 ----> anos,\n",
      "3 ----> o\n",
      "5 ----> que\n",
      "7230 ----> prejudica\n",
      "78 ----> seu\n",
      "2914 ----> funcionamento,\n",
      "70 ----> diz\n",
      "2900 ----> astray.\n",
      "942 ----> \"os\n",
      "1946 ----> níveis\n",
      "440 ----> diferentes\n",
      "26 ----> são\n",
      "138 ----> porque\n",
      "19 ----> as\n",
      "232 ----> crianças\n",
      "682 ----> tiveram\n",
      "15 ----> uma\n",
      "2915 ----> exposição\n",
      "804 ----> menor\n",
      "24 ----> ao\n",
      "213 ----> vírus\n",
      "34 ----> ou\n",
      "138 ----> porque\n",
      "17 ----> não\n",
      "2905 ----> desenvolveram\n",
      "7231 ----> anticorpos?\n",
      "88 ----> essa\n",
      "16 ----> é\n",
      "15 ----> uma\n",
      "7232 ----> interrogação\n",
      "5 ----> que\n",
      "7233 ----> fica.\"\n",
      "43 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "12 ----> por\n",
      "11 ----> que\n",
      "9 ----> a\n",
      "231 ----> espanha\n",
      "54 ----> mostra\n",
      "11 ----> que\n",
      "496 ----> deveremos\n",
      "104 ----> ter\n",
      "22 ----> mais\n",
      "232 ----> quarentenas\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nj1kuh2hirSd"
   },
   "outputs": [],
   "source": [
    "# criando o dataset\n",
    "\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 16\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 200\n",
    "units = 1024\n",
    "\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "VUSjdb22jMoU",
    "outputId": "13d72eac-e8f1-4290-d188-9cea435992d7"
   },
   "outputs": [],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hc8hA8NNjSuc"
   },
   "outputs": [],
   "source": [
    "# bloco do encoder\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "fmvm1TGQjUGz",
    "outputId": "e01c201d-eaf7-4ccd-e72f-96d2ffb14ffe"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s61ZzdrjjaqT"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    # query hidden state shape == (batch_size, hidden size)\n",
    "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # values shape == (batch_size, max_len, hidden size)\n",
    "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "    query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "3bDQHCeUjdeb",
    "outputId": "1aee5ebe-a5b4-4f7f-ce89-031a0a8ec46f"
   },
   "outputs": [],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3vSHrnojjE7"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "nSZVg3F7jk5T",
    "outputId": "a33c5576-1010-4308-bc2e-38eff0219f1d"
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48arRwVYjqIL"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQriFWRIjryT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccxVaiRLj51C"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JV-hYcnOkBOa",
    "outputId": "f8820948-c8cd-4c3f-f36a-26c6730584bc"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "EPOCHS = 200\n",
    "\n",
    "loss_acumulado = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  loss_acumulado.append(total_loss / steps_per_epoch)\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "ZAM9TMv8ND4f",
    "outputId": "fd0e80bb-e744-4aca-e01f-a6c32d86c9e2"
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_acumulado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZXcFnrz984X"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "  sentence = sentence.lower()\n",
    "\n",
    "  inputs = inp_lang.texts_to_sequences([sentence])\n",
    "\n",
    "  #inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "\n",
    "  print(inputs)\n",
    "\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  print(inputs.shape)\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "  print(inputs.shape)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "    # storing the attention weights to plot later on\n",
    "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if targ_lang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence, attention_plot\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence, attention_plot\n",
    "\n",
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "  fig = plt.figure(figsize=(10,10))\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "  ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "  fontdict = {'fontsize': 14}\n",
    "\n",
    "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def translate(sentence):\n",
    "  result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))\n",
    "\n",
    "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "cfJgm7-b-hx6",
    "outputId": "d8d927fa-4666-43ae-fdd8-9d87f17b8549"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "mensagem = '<start> ' + data.texto.tolist()[15] + ' <end>'\n",
    "print(mensagem)\n",
    "\n",
    "translate(mensagem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Sumarização Abstrata.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
